// linkedin-scraper-stealth.js
// Versi√≥n corregida para Railway con mejor manejo de errores

const puppeteer = require('puppeteer-extra');
const { execSync } = require('child_process');
const fs = require('fs');
const StealthPlugin = require('puppeteer-extra-plugin-stealth');
const Airtable = require('airtable');
const cron = require('node-cron');
const express = require('express'); // <<-- A√ëADIDO: Express para API

// Activar plugin stealth
puppeteer.use(StealthPlugin());

const CONFIG = {
  AIRTABLE_API_KEY: process.env.AIRTABLE_API_KEY,
// ... (resto de CONFIG es igual) ...
  PROXY_PASSWORD: process.env.PROXY_PASSWORD,
};

const base = new Airtable({ apiKey: CONFIG.AIRTABLE_API_KEY }).base(CONFIG.AIRTABLE_BASE_ID);

// ----------------------------------------------------
// -- INICIO DE C√ìDIGO JS ORIGINAL (sin cambios en funciones)
// ----------------------------------------------------

// ... (El c√≥digo de UTILIDADES, GESTI√ìN DE COOKIES, AIRTABLE, NAVEGACI√ìN, SCRAPING debe permanecer igual) ...

// Funci√≥n findChromePath (La mantenemos, pero la deshabilitamos para la ejecuci√≥n de Puppeteer)
function findChromePath() {
    // ... (Mantener la funci√≥n original, pero ya no la usaremos directamente en puppeteer.launch)
}

// ... (El resto de las funciones auxiliares hasta runScraper) ...

// ========================================
// FUNCION PRINCIPAL MODIFICADA
// ========================================

// Modificamos runScraper para aceptar par√°metros de la API (si es necesario)
// Por simplicidad, por ahora la dejamos sin par√°metros
async function runScraper() {
  log('üöÄ Iniciando scraper de LinkedIn con Stealth...');
  
  let browser;
  let success = false;
  let totalNewPosts = 0;
  let error = null;
  
  try {
    await checkCookieExpiration();
    
    const profiles = await getActiveProfiles();
    
    // ... (resto de la l√≥gica de runScraper es la misma) ...
    
    // Aqu√≠ es donde corregimos la detecci√≥n de Chrome:
    // Ya que instalamos Chromium en el Dockerfile, NO buscamos con rutas r√≠gidas.
    // Solo permitimos que Puppeteer use la variable de entorno PUPPETEER_EXECUTABLE_PATH
    
    const browserArgs = [
      '--no-sandbox',
      // ... (resto de browserArgs es igual) ...
    ];

    if (CONFIG.PROXY_HOST && CONFIG.PROXY_PORT) {
      browserArgs.push(`--proxy-server=${CONFIG.PROXY_HOST}:${CONFIG.PROXY_PORT}`);
      log(`üåê Usando proxy: ${CONFIG.PROXY_HOST}:${CONFIG.PROXY_PORT}`);
    }
    
    // CORRECCI√ìN CR√çTICA: Quitamos el findChromePath y usamos la variable de Dockerfile
    // const chromePath = findChromePath(); // <-- COMENTADO/ELIMINADO
    
    browser = await puppeteer.launch({
      headless: 'new',
      args: browserArgs,
      // La ruta ejecutable ahora viene del Dockerfile: PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium
      ignoreHTTPSErrors: true,
      dumpio: false, 
      defaultViewport: null
    });
    
    // ... (resto del c√≥digo de scraping hasta el final de runScraper) ...
    
  } catch (err) {
    error = err.message;
    log(`‚ùå Error fatal: ${error}`, 'error');
    throw err; // Es importante lanzar el error para el bloque de la API
  } finally {
    if (browser) {
      await browser.close();
      log('üîí Browser cerrado');
    }
    
    await logScraperRun(success, totalNewPosts, error);
    return { success, totalNewPosts, error }; // Devolver el resultado
  }
}

// ========================================
// EJECUCI√ìN PRINCIPAL (CON API)
// ========================================

const app = express();
const PORT = process.env.PORT || 3000;
app.use(express.json());

// 1. ENDPOINT PARA N8N (Dispara el scraper bajo demanda)
app.post('/scrape-on-demand', async (req, res) => {
    log('‚ö°Ô∏è API: Llamada de n8n recibida para scraping bajo demanda');
    
    // Responder inmediatamente (202 Accepted) para evitar timeouts de n8n, 
    // ya que el scraping es una tarea larga.
    res.status(202).send({ 
        status: 'Processing', 
        message: 'Scraper task initiated. Check logs for results.' 
    });

    // Ejecuta el scraper en background
    runScraper().catch(err => {
        log(`Error en API run: ${err.message}`, 'error');
    });
});

// 2. INICIO DEL SERVICIO
app.listen(PORT, () => {
    log(`‚úÖ Servidor API Express corriendo en puerto ${PORT}`);
});

// 3. PROGRAMAR EJECUCIONES PERI√ìDICAS (mantenemos el cron)
log(`‚è±Ô∏è Cron configurado: ${CONFIG.CRON_SCHEDULE}`);
cron.schedule(CONFIG.CRON_SCHEDULE, () => {
    log('‚è∞ Ejecutando tarea programada...');
    runScraper().catch(err => {
        log(`Error en tarea programada: ${err.message}`, 'error');
    });
});

// Inicializaci√≥n de logs
log('üì± Aplicaci√≥n iniciada con Stealth Plugin');
log('üîî Sistema de monitoreo de cookies activo');

if (!process.env.LINKEDIN_COOKIES || !process.env.AIRTABLE_API_KEY || !process.env.AIRTABLE_BASE_ID) {
    log('‚ùå ERROR: Variables de entorno cr√≠ticas faltantes.', 'error');
    log('üí° Por favor, configura LINKEDIN_COOKIES, AIRTABLE_API_KEY y AIRTABLE_BASE_ID en Railway.', 'warning');
    // NOTA: No hacemos process.exit(1) aqu√≠ porque queremos que Express inicie para depuraci√≥n.
}
